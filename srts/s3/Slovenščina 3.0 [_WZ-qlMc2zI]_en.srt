1
00:00:00,000 --> 00:00:03,000
A few words about the new episode of Ogrodije

2
00:00:03,000 --> 00:00:06,000
and about the new season of Make It.

3
00:00:12,000 --> 00:00:16,000
Greetings to all of you in the new episode of Ogrodije

4
00:00:16,000 --> 00:00:20,000
and greetings to all of you at the Make It conference.

5
00:00:20,000 --> 00:00:25,000
Today we are filming this episode of Ogrodije in front of the live audience,

6
00:00:25,000 --> 00:00:30,000
but it won't be anything less interesting and technically advanced.

7
00:00:31,000 --> 00:00:35,000
Today we have an interesting topic,

8
00:00:35,000 --> 00:00:38,000
which we have named Slovenian 3.0.

9
00:00:39,000 --> 00:00:48,000
This topic has become very relevant in the last few years.

10
00:00:48,000 --> 00:01:00,000
We are in the third phase of language technologies,

11
00:01:00,000 --> 00:01:03,000
and we hope that we have debunked it.

12
00:01:03,000 --> 00:01:06,000
When we started with Slovenian,

13
00:01:06,000 --> 00:01:12,000
Slovenian was called a written word, a spoken word,

14
00:01:12,000 --> 00:01:15,000
and we recorded some points.

15
00:01:15,000 --> 00:01:18,000
Then we tried to digitize it with technology,

16
00:01:18,000 --> 00:01:21,000
we have dictionaries, translators and so on.

17
00:01:21,000 --> 00:01:25,000
Now we are working with technologies

18
00:01:25,000 --> 00:01:29,000
that were based on large language models

19
00:01:29,000 --> 00:01:32,000
in the third stage of Slovenian,

20
00:01:32,000 --> 00:01:35,000
and we have also taken our word for it.

21
00:01:35,000 --> 00:01:40,000
Today we are joined by Prof. Dr. Marko Bajec from FRIA,

22
00:01:40,000 --> 00:01:45,000
who is involved in various laboratories and projects

23
00:01:45,000 --> 00:01:48,000
with modern language technologies.

24
00:01:48,000 --> 00:01:52,000
I hope that we will explore this area together,

25
00:01:52,000 --> 00:01:55,000
and that we will go not only superficially,

26
00:01:55,000 --> 00:01:58,000
but also a little deeper into these concepts.

27
00:01:58,000 --> 00:02:01,000
I forgot something, DraÅ¾, before...

28
00:02:01,000 --> 00:02:04,000
We have agreed that we will talk,

29
00:02:04,000 --> 00:02:07,000
so it won't be strange to anyone.

30
00:02:07,000 --> 00:02:10,000
We are the same age.

31
00:02:10,000 --> 00:02:13,000
Yes, we are the same age as engineers.

32
00:02:13,000 --> 00:02:16,000
I would ask you to introduce yourself,

33
00:02:16,000 --> 00:02:21,000
so that we can properly introduce ourselves.

34
00:02:21,000 --> 00:02:25,000
Thank you. First of all, hello to everyone in the audience.

35
00:02:25,000 --> 00:02:29,000
I am Marko Bajec from the Faculty of Journalism and Informatics.

36
00:02:29,000 --> 00:02:32,000
I teach subjects in the field of data databases,

37
00:02:32,000 --> 00:02:36,000
which of course also includes me from the previous conference.

38
00:02:36,000 --> 00:02:40,000
I also develop information systems and technologies

39
00:02:40,000 --> 00:02:43,000
that are connected to speech and language.

40
00:02:43,000 --> 00:02:47,000
I lead a laboratory for data technologies,

41
00:02:47,000 --> 00:02:50,000
and in recent years we have been dealing a lot

42
00:02:50,000 --> 00:02:53,000
with speech and language technologies.

43
00:02:53,000 --> 00:02:56,000
We are trying to bring Slovenia to a level

44
00:02:56,000 --> 00:02:59,000
where there is no digitalization,

45
00:02:59,000 --> 00:03:02,000
to a point where in the future

46
00:03:02,000 --> 00:03:05,000
we would know a lot more.

47
00:03:05,000 --> 00:03:08,000
Today we know that we expect

48
00:03:08,000 --> 00:03:11,000
that most of the communication between humans

49
00:03:11,000 --> 00:03:14,000
will be on the right speech.

50
00:03:14,000 --> 00:03:17,000
If we don't take care of our language,

51
00:03:17,000 --> 00:03:20,000
even in the digital sense,

52
00:03:20,000 --> 00:03:23,000
it can happen to us very quickly

53
00:03:23,000 --> 00:03:26,000
that we will be very disappointed

54
00:03:26,000 --> 00:03:29,000
to see what solutions others have,

55
00:03:29,000 --> 00:03:32,000
and that is why I am very happy

56
00:03:32,000 --> 00:03:35,000
to be able to share these things.

57
00:03:35,000 --> 00:03:38,000
Great. I wanted to start this debate

58
00:03:38,000 --> 00:03:41,000
with a powerful question.

59
00:03:41,000 --> 00:03:44,000
How far are we technologically

60
00:03:44,000 --> 00:03:47,000
when we can,

61
00:03:47,000 --> 00:03:50,000
I said technology,

62
00:03:50,000 --> 00:03:53,000
I will ask the question again,

63
00:03:53,000 --> 00:03:56,000
tell our medical symptoms

64
00:03:56,000 --> 00:03:59,000
in Slovenia,

65
00:03:59,000 --> 00:04:02,000
and we will get back a diagnosis

66
00:04:02,000 --> 00:04:05,000
that we can trust.

67
00:04:05,000 --> 00:04:08,000
Is this too powerful a question?

68
00:04:08,000 --> 00:04:11,000
The question is fine,

69
00:04:11,000 --> 00:04:14,000
but it depends on how I will answer it.

70
00:04:14,000 --> 00:04:17,000
First of all,

71
00:04:17,000 --> 00:04:20,000
I belong to a more conservative group

72
00:04:20,000 --> 00:04:23,000
of researchers and scientists,

73
00:04:23,000 --> 00:04:26,000
so I am not particularly

74
00:04:26,000 --> 00:04:29,000
technologically inspired.

75
00:04:29,000 --> 00:04:32,000
But given the fact that most of you

76
00:04:32,000 --> 00:04:35,000
have technical background,

77
00:04:35,000 --> 00:04:38,000
I don't think I need to gather words.

78
00:04:38,000 --> 00:04:41,000
That is why I will say

79
00:04:41,000 --> 00:04:44,000
what I see in reality,

80
00:04:44,000 --> 00:04:47,000
but I don't want to give everyone the same answer.

81
00:04:47,000 --> 00:04:50,000
I think that we are already very far

82
00:04:50,000 --> 00:04:53,000
and the solutions that big language models have

83
00:04:53,000 --> 00:04:56,000
are already a very good assistant.

84
00:04:56,000 --> 00:04:59,000
We work in tandem

85
00:04:59,000 --> 00:05:02,000
if we trust them to decide for themselves.

86
00:05:02,000 --> 00:05:05,000
The same is true in the field of medicine.

87
00:05:05,000 --> 00:05:08,000
I think that the solutions

88
00:05:08,000 --> 00:05:11,000
that the health care provider consults

89
00:05:11,000 --> 00:05:14,000
can be very useful,

90
00:05:14,000 --> 00:05:17,000
because they have in themselves

91
00:05:17,000 --> 00:05:20,000
and in other language models

92
00:05:20,000 --> 00:05:23,000
a huge amount of knowledge,

93
00:05:23,000 --> 00:05:26,000
which the health care provider

94
00:05:26,000 --> 00:05:29,000
can't even process.

95
00:05:29,000 --> 00:05:32,000
They can be helpful,

96
00:05:32,000 --> 00:05:35,000
but the final decision and interpretation

97
00:05:35,000 --> 00:05:38,000
must be made by a human being.

98
00:05:38,000 --> 00:05:41,000
In your question,

99
00:05:41,000 --> 00:05:44,000
such solutions already exist.

100
00:05:44,000 --> 00:05:47,000
If you check them,

101
00:05:47,000 --> 00:05:50,000
you will see that they are mostly

102
00:05:50,000 --> 00:05:53,000
in the direction of the health care provider,

103
00:05:53,000 --> 00:05:56,000
because it is unified.

104
00:05:56,000 --> 00:05:59,000
One of the difficulties in big language models

105
00:05:59,000 --> 00:06:02,000
is that we met them

106
00:06:02,000 --> 00:06:05,000
through chatbots,

107
00:06:05,000 --> 00:06:08,000
where people started to see

108
00:06:08,000 --> 00:06:11,000
that they can be asked and answered.

109
00:06:11,000 --> 00:06:14,000
How do you hide the fact

110
00:06:14,000 --> 00:06:17,000
that there is a lot of knowledge encoded in them?

111
00:06:17,000 --> 00:06:20,000
Is this preparation of knowledge,

112
00:06:20,000 --> 00:06:23,000
cleaning of knowledge,

113
00:06:23,000 --> 00:06:26,000
is it a good description

114
00:06:26,000 --> 00:06:29,000
of these technologies?

115
00:06:29,000 --> 00:06:32,000
Especially since most people

116
00:06:32,000 --> 00:06:35,000
receive big language models

117
00:06:35,000 --> 00:06:38,000
through chatbots,

118
00:06:38,000 --> 00:06:41,000
such as chatGPT,

119
00:06:41,000 --> 00:06:44,000
which today are much more

120
00:06:44,000 --> 00:06:47,000
than just a language model.

121
00:06:47,000 --> 00:06:50,000
It is true that the language model

122
00:06:50,000 --> 00:06:53,000
is the backbone,

123
00:06:53,000 --> 00:06:56,000
but even if you have studied

124
00:06:56,000 --> 00:06:59,000
how they were taught,

125
00:06:59,000 --> 00:07:02,000
only the first four or five phases

126
00:07:02,000 --> 00:07:05,000
were dedicated to the language model.

127
00:07:05,000 --> 00:07:08,000
When we approach people in this way,

128
00:07:08,000 --> 00:07:11,000
we think that all language models

129
00:07:11,000 --> 00:07:14,000
are capable of this.

130
00:07:14,000 --> 00:07:17,000
Then there is a great disappointment

131
00:07:17,000 --> 00:07:20,000
when an open source model comes

132
00:07:20,000 --> 00:07:23,000
and does not know how to answer all the questions.

133
00:07:23,000 --> 00:07:26,000
If you ask chatGPT,

134
00:07:26,000 --> 00:07:29,000
it will know how to answer anything,

135
00:07:29,000 --> 00:07:32,000
so it seems quite sensible.

136
00:07:32,000 --> 00:07:35,000
It will also know how to answer questions.

137
00:07:35,000 --> 00:07:38,000
For this part,

138
00:07:38,000 --> 00:07:41,000
chatGPT took a lot of time

139
00:07:41,000 --> 00:07:44,000
and included 10,000 engineers

140
00:07:44,000 --> 00:07:47,000
who helped with supervised learning

141
00:07:47,000 --> 00:07:50,000
and used techniques

142
00:07:50,000 --> 00:07:53,000
such as sub-combat learning,

143
00:07:53,000 --> 00:07:56,000
which is not part of the learning

144
00:07:56,000 --> 00:07:59,000
of the language model at the beginning.

145
00:07:59,000 --> 00:08:02,000
Let's go even deeper.

146
00:08:02,000 --> 00:08:05,000
As engineers,

147
00:08:05,000 --> 00:08:08,000
we often use apps

148
00:08:08,000 --> 00:08:11,000
that are available to us,

149
00:08:11,000 --> 00:08:14,000
such as international corporations

150
00:08:14,000 --> 00:08:17,000
and large companies.

151
00:08:17,000 --> 00:08:20,000
I am not interested in this.

152
00:08:20,000 --> 00:08:23,000
I have no feeling

153
00:08:23,000 --> 00:08:26,000
if there is such a model

154
00:08:26,000 --> 00:08:29,000
in Slovakia.

155
00:08:29,000 --> 00:08:32,000
I have no feeling

156
00:08:32,000 --> 00:08:35,000
if we know chatGPT,

157
00:08:35,000 --> 00:08:38,000
we can make an app

158
00:08:38,000 --> 00:08:41,000
and call it in Slovakia,

159
00:08:41,000 --> 00:08:44,000
but exclusively in Slovakia.

160
00:08:44,000 --> 00:08:47,000
I can't imagine where we are.

161
00:08:47,000 --> 00:08:50,000
My only experience

162
00:08:50,000 --> 00:08:53,000
was a medical examination

163
00:08:53,000 --> 00:08:56,000
and a call

164
00:08:56,000 --> 00:08:59,000
where the model wrote what it heard.

165
00:08:59,000 --> 00:09:02,000
The patient's wish

166
00:09:02,000 --> 00:09:05,000
would be to automatically write

167
00:09:05,000 --> 00:09:08,000
a diagnosis in such a structure

168
00:09:08,000 --> 00:09:11,000
that he would not need it.

169
00:09:11,000 --> 00:09:14,000
There is an additional possibility

170
00:09:14,000 --> 00:09:17,000
to enable a language model.

171
00:09:17,000 --> 00:09:20,000
If I go back to what I said,

172
00:09:20,000 --> 00:09:23,000
I have heard many times

173
00:09:23,000 --> 00:09:26,000
whether we should be afraid

174
00:09:26,000 --> 00:09:29,000
to give birth to Slavs.

175
00:09:29,000 --> 00:09:32,000
They will eat us anyway.

176
00:09:32,000 --> 00:09:35,000
Slavs are not interested

177
00:09:35,000 --> 00:09:38,000
in the market.

178
00:09:38,000 --> 00:09:41,000
We all know this,

179
00:09:41,000 --> 00:09:44,000
but we don't want to stay.

180
00:09:44,000 --> 00:09:47,000
I think they will eat us

181
00:09:47,000 --> 00:09:50,000
Let's take chatGPT,

182
00:09:50,000 --> 00:09:53,000
which was named Trojko.

183
00:09:53,000 --> 00:09:56,000
As you probably guessed,

184
00:09:56,000 --> 00:09:59,000
because it is a public data,

185
00:09:59,000 --> 00:10:02,000
its model has 175 billion parameters.

186
00:10:02,000 --> 00:10:05,000
If we look at how many buildings

187
00:10:05,000 --> 00:10:08,000
are needed for such a model,

188
00:10:08,000 --> 00:10:11,000
there was a lot of research on this topic,

189
00:10:11,000 --> 00:10:14,000
how many tokens,

190
00:10:14,000 --> 00:10:17,000
are needed for one parameter.

191
00:10:17,000 --> 00:10:20,000
There are measurements from 2 to 200.

192
00:10:20,000 --> 00:10:23,000
For example, from 2 tokens to 200 tokens

193
00:10:23,000 --> 00:10:26,000
are needed for one parameter.

194
00:10:26,000 --> 00:10:29,000
If we take 10,

195
00:10:29,000 --> 00:10:32,000
it means that we need

196
00:10:32,000 --> 00:10:35,000
1750 billion tokens

197
00:10:35,000 --> 00:10:38,000
for such a model.

198
00:10:38,000 --> 00:10:41,000
A book typically has

199
00:10:41,000 --> 00:10:44,000
50,000 to 100,000 words.

200
00:10:44,000 --> 00:10:47,000
To explain the difference

201
00:10:47,000 --> 00:10:50,000
between words and tokens,

202
00:10:50,000 --> 00:10:53,000
big language models are not taught.

203
00:10:53,000 --> 00:10:56,000
Their vocabulary is not just words,

204
00:10:56,000 --> 00:10:59,000
but also part words.

205
00:10:59,000 --> 00:11:02,000
In this way, they can cover words

206
00:11:02,000 --> 00:11:05,000
that do not yet exist in the vocabulary.

207
00:11:05,000 --> 00:11:08,000
New names and so on

208
00:11:08,000 --> 00:11:11,000
typically have 50,000 to 100,000 tokens.

209
00:11:11,000 --> 00:11:14,000
20% to 30% of them are words,

210
00:11:14,000 --> 00:11:17,000
but they are the thickest words.

211
00:11:17,000 --> 00:11:20,000
If we took a book and tokenized it,

212
00:11:20,000 --> 00:11:23,000
it would be easy to show that

213
00:11:23,000 --> 00:11:26,000
50% to 70% is tokenized with words,

214
00:11:26,000 --> 00:11:29,000
and the rest with parts.

215
00:11:29,000 --> 00:11:32,000
If we calculate this,

216
00:11:32,000 --> 00:11:35,000
we will come to 1.5-1.6 tokens per word.

217
00:11:35,000 --> 00:11:38,000
If we take 50,000 to 100,000 words,

218
00:11:38,000 --> 00:11:41,000
we get 80,000 words,

219
00:11:41,000 --> 00:11:44,000
multiplied by 1.6 or 1.5,

220
00:11:44,000 --> 00:11:47,000
we get about 130,000 words.

221
00:11:47,000 --> 00:11:50,000
If we divide these parameters,

222
00:11:50,000 --> 00:11:53,000
1,370,000,

223
00:11:53,000 --> 00:11:56,000
we get about 10 million books.

224
00:11:56,000 --> 00:11:59,000
Slovenia produces 5,000 books every year.

225
00:11:59,000 --> 00:12:02,000
We will produce books for 3,000 years

226
00:12:02,000 --> 00:12:05,000
and we will have this many books.

227
00:12:05,000 --> 00:12:08,000
This is just for the sake of feeling

228
00:12:08,000 --> 00:12:11,000
what large quantities mean.

229
00:12:11,000 --> 00:12:14,000
In the case of LMA,

230
00:12:14,000 --> 00:12:17,000
books accounted for 20% of the population,

231
00:12:17,000 --> 00:12:20,000
at least.

232
00:12:20,000 --> 00:12:23,000
I don't know for sure,

233
00:12:23,000 --> 00:12:26,000
but if we want diversity,

234
00:12:26,000 --> 00:12:29,000
we have to include a lot of other buildings,

235
00:12:29,000 --> 00:12:32,000
and where in Slovenia to get it.

236
00:12:32,000 --> 00:12:35,000
What we are offered now

237
00:12:35,000 --> 00:12:38,000
by larger language models,

238
00:12:38,000 --> 00:12:41,000
because they also know Slovenian,

239
00:12:41,000 --> 00:12:44,000
is what can be taken from the Internet.

240
00:12:44,000 --> 00:12:47,000
Projects that crawl from the Internet

241
00:12:47,000 --> 00:12:50,000
are mass-produced,

242
00:12:50,000 --> 00:12:53,000
and as much as Slovenian is included,

243
00:12:53,000 --> 00:12:56,000
it is included.

244
00:12:56,000 --> 00:12:59,000
There is nothing controlled.

245
00:12:59,000 --> 00:13:02,000
In the end, the first models

246
00:13:02,000 --> 00:13:05,000
were really taught by a group of masses.

247
00:13:05,000 --> 00:13:08,000
Now it is clear that in addition to the mass,

248
00:13:08,000 --> 00:13:11,000
there is also an important quality.

249
00:13:11,000 --> 00:13:14,000
A very good language model

250
00:13:14,000 --> 00:13:17,000
for the field of medicine

251
00:13:17,000 --> 00:13:20,000
is Google's MatPalm 2.

252
00:13:20,000 --> 00:13:23,000
I think it has passed the tests

253
00:13:23,000 --> 00:13:26,000
of all medical institutions in the US

254
00:13:26,000 --> 00:13:29,000
to get a license.

255
00:13:29,000 --> 00:13:32,000
They had 60% and with this model

256
00:13:32,000 --> 00:13:35,000
they got 87% of the correct answers.

257
00:13:35,000 --> 00:13:38,000
It is very important

258
00:13:38,000 --> 00:13:41,000
what beliefs are.

259
00:13:41,000 --> 00:13:44,000
Beliefs are completely superstitious.

260
00:13:44,000 --> 00:13:47,000
We are talking about larger language models

261
00:13:47,000 --> 00:13:50,000
where some Slovenian is accidentally included.

262
00:13:50,000 --> 00:13:53,000
Then the question is

263
00:13:53,000 --> 00:13:56,000
if they are superstitious.

264
00:13:56,000 --> 00:13:59,000
It doesn't matter that we do something in Slovenian.

265
00:13:59,000 --> 00:14:02,000
If you delve into how language models work,

266
00:14:02,000 --> 00:14:05,000
then you will realize

267
00:14:05,000 --> 00:14:08,000
that it is very important

268
00:14:08,000 --> 00:14:11,000
what we give at the source.

269
00:14:11,000 --> 00:14:14,000
Large language models were taught by the masses.

270
00:14:14,000 --> 00:14:17,000
Grammar, syntax,

271
00:14:17,000 --> 00:14:20,000
pragmatics, semantics,

272
00:14:20,000 --> 00:14:23,000
extremely large linguistic values

273
00:14:23,000 --> 00:14:26,000
that we don't even know how to name.

274
00:14:26,000 --> 00:14:29,000
They are included in neural networks.

275
00:14:29,000 --> 00:14:32,000
But they are included from a large mass

276
00:14:32,000 --> 00:14:35,000
and a variety of constructions.

277
00:14:35,000 --> 00:14:38,000
If we translate it correctly,

278
00:14:38,000 --> 00:14:41,000
we will get an extremely poor language.

279
00:14:41,000 --> 00:14:44,000
Subsequently, the model will not evolve.

280
00:14:45,000 --> 00:14:48,000
I think that the only way

281
00:14:48,000 --> 00:14:51,000
if we want to take care of Slovenia

282
00:14:51,000 --> 00:14:54,000
is to make sure that as many people as possible love us.

283
00:14:54,000 --> 00:14:57,000
Not only for Slovenia,

284
00:14:57,000 --> 00:15:00,000
but for the whole family of Slavic languages.

285
00:15:00,000 --> 00:15:03,000
They are all somewhat similar

286
00:15:03,000 --> 00:15:06,000
and the model can be learned from common properties.

287
00:15:06,000 --> 00:15:09,000
And that these people love us as much as possible.

288
00:15:09,000 --> 00:15:12,000
We are doing our best

289
00:15:12,000 --> 00:15:15,000
and we are conducting a project

290
00:15:15,000 --> 00:15:18,000
at the Faculty of Criminology

291
00:15:18,000 --> 00:15:21,000
which is intended to track

292
00:15:21,000 --> 00:15:24,000
how the most wanted person

293
00:15:24,000 --> 00:15:27,000
comes to the data

294
00:15:27,000 --> 00:15:30,000
in these special categories

295
00:15:30,000 --> 00:15:33,000
in order to make a model

296
00:15:33,000 --> 00:15:36,000
that can be used in the public good

297
00:15:36,000 --> 00:15:39,000
and does not reveal more of the data

298
00:15:39,000 --> 00:15:42,000
because we have very tight deadlines here.

299
00:15:42,000 --> 00:15:45,000
Just two years ago,

300
00:15:45,000 --> 00:15:48,000
I think that AMZS and Zvezda PotroÅ¡niko

301
00:15:48,000 --> 00:15:51,000
encouraged the State Assembly

302
00:15:51,000 --> 00:15:54,000
to do something

303
00:15:54,000 --> 00:15:57,000
because the vehicles we buy in Slovenia

304
00:15:57,000 --> 00:16:00,000
do not speak Slovenian.

305
00:16:00,000 --> 00:16:03,000
We do not have to talk to them in Slovenian.

306
00:16:03,000 --> 00:16:06,000
Why not?

307
00:16:06,000 --> 00:16:09,000
When we turned to VW

308
00:16:09,000 --> 00:16:12,000
and so on,

309
00:16:12,000 --> 00:16:15,000
we finally came to Germany

310
00:16:15,000 --> 00:16:18,000
and they said,

311
00:16:18,000 --> 00:16:21,000
OK, give us the data.

312
00:16:21,000 --> 00:16:24,000
Which data can we give now?

313
00:16:24,000 --> 00:16:27,000
During the preparation of the episode,

314
00:16:27,000 --> 00:16:30,000
we talked and you mentioned

315
00:16:30,000 --> 00:16:33,000
that we will change the legislation

316
00:16:33,000 --> 00:16:36,000
in order to attract more believers.

317
00:16:36,000 --> 00:16:39,000
Will this be an initiative on a European level

318
00:16:39,000 --> 00:16:42,000
or a Slovenian one?

319
00:16:42,000 --> 00:16:45,000
Will it be necessary to change the legislation

320
00:16:45,000 --> 00:16:48,000
in order to attract more believers?

321
00:16:48,000 --> 00:16:51,000
The law on risk activities has already changed

322
00:16:51,000 --> 00:16:54,000
but it allows different interpretations.

323
00:16:54,000 --> 00:16:57,000
I can give you an example from our project.

324
00:16:57,000 --> 00:17:00,000
We invited people to the database

325
00:17:00,000 --> 00:17:03,000
where a lot of information is recorded

326
00:17:03,000 --> 00:17:06,000
and then they are transcribed.

327
00:17:06,000 --> 00:17:09,000
This is a great educational tool

328
00:17:09,000 --> 00:17:12,000
and it is gaining a lot of popularity.

329
00:17:12,000 --> 00:17:15,000
For example, it is gaining popularity

330
00:17:15,000 --> 00:17:18,000
every year.

331
00:17:18,000 --> 00:17:21,000
But we did not receive this data

332
00:17:21,000 --> 00:17:24,000
because according to the law

333
00:17:24,000 --> 00:17:27,000
we have a priority.

334
00:17:27,000 --> 00:17:30,000
But on the other hand,

335
00:17:30,000 --> 00:17:33,000
the owner or the data manager

336
00:17:33,000 --> 00:17:36,000
cannot force so much work

337
00:17:36,000 --> 00:17:39,000
that it does not benefit

338
00:17:39,000 --> 00:17:42,000
what we will do.

339
00:17:42,000 --> 00:17:45,000
I understand the answer of some people

340
00:17:45,000 --> 00:17:48,000
who have archived this data

341
00:17:48,000 --> 00:17:51,000
in a very different way

342
00:17:51,000 --> 00:17:54,000
and did not spend much money

343
00:17:54,000 --> 00:17:57,000
but I want to say that the legislation

344
00:17:57,000 --> 00:18:00,000
is moving in this direction

345
00:18:00,000 --> 00:18:03,000
and it gives us more opportunities

346
00:18:03,000 --> 00:18:06,000
but it is still difficult to get the data.

347
00:18:06,000 --> 00:18:09,000
We can ask the RTV for the data

348
00:18:09,000 --> 00:18:12,000
but we will not get them again

349
00:18:12,000 --> 00:18:15,000
because they are copyrighted.

350
00:18:15,000 --> 00:18:18,000
What I would like to say

351
00:18:18,000 --> 00:18:21,000
is that the minority of Slovenia

352
00:18:21,000 --> 00:18:24,000
is a bit more innovative

353
00:18:24,000 --> 00:18:27,000
and a bit more open.

354
00:18:27,000 --> 00:18:30,000
For all of us who need it.

355
00:18:30,000 --> 00:18:33,000
A big part of the language technologies

356
00:18:33,000 --> 00:18:36,000
are the corpuses.

357
00:18:36,000 --> 00:18:39,000
Can you tell us

358
00:18:39,000 --> 00:18:42,000
what are the corpuses

359
00:18:42,000 --> 00:18:45,000
and how are they prepared

360
00:18:45,000 --> 00:18:48,000
and if they are still used

361
00:18:48,000 --> 00:18:51,000
in the future?

362
00:18:51,000 --> 00:18:54,000
The corpuses are very different things

363
00:18:54,000 --> 00:18:57,000
and that is a question for the linguist.

364
00:18:57,000 --> 00:19:00,000
But I can tell you what the corpuses are

365
00:19:00,000 --> 00:19:03,000
and where we use them.

366
00:19:03,000 --> 00:19:06,000
When we practice the natural language

367
00:19:06,000 --> 00:19:09,000
it is very important that we know

368
00:19:09,000 --> 00:19:12,000
the whole vocabulary and that we know

369
00:19:12,000 --> 00:19:15,000
what each word has,

370
00:19:15,000 --> 00:19:18,000
because then we can teach the system

371
00:19:18,000 --> 00:19:21,000
that when they practice the natural language

372
00:19:21,000 --> 00:19:24,000
they know from the data

373
00:19:24,000 --> 00:19:27,000
what the literal meaning of a word is

374
00:19:27,000 --> 00:19:30,000
and from that they know how to come to

375
00:19:30,000 --> 00:19:33,000
the correct sentences

376
00:19:33,000 --> 00:19:36,000
or to solve the tasks that we give them.

377
00:19:36,000 --> 00:19:39,000
That is why we need different corpuses.

378
00:19:39,000 --> 00:19:42,000
But are they also used in building LLMs?

379
00:19:42,000 --> 00:19:45,000
Those are additional knowledge

380
00:19:45,000 --> 00:19:48,000
that we can add to the learning of LLMs.

381
00:19:48,000 --> 00:19:51,000
In the case of LLMs,

382
00:19:51,000 --> 00:19:54,000
as they were in the beginning of their learning,

383
00:19:54,000 --> 00:19:57,000
it is based on a huge amount of data.

384
00:19:57,000 --> 00:20:00,000
LLMs are based on

385
00:20:00,000 --> 00:20:03,000
generative artificial intelligence.

386
00:20:03,000 --> 00:20:06,000
They try to understand

387
00:20:06,000 --> 00:20:09,000
all the data

388
00:20:09,000 --> 00:20:12,000
and all the linguistic values

389
00:20:12,000 --> 00:20:15,000
in the words

390
00:20:15,000 --> 00:20:18,000
to make the best prediction

391
00:20:18,000 --> 00:20:21,000
of the next sentence

392
00:20:21,000 --> 00:20:24,000
from the context they have.

393
00:20:24,000 --> 00:20:27,000
Generative language models

394
00:20:27,000 --> 00:20:30,000
generate the next sentence

395
00:20:30,000 --> 00:20:33,000
based on the prompt you gave them,

396
00:20:33,000 --> 00:20:36,000
but not all the sentences.

397
00:20:37,000 --> 00:20:40,000
When we were preparing for this episode,

398
00:20:40,000 --> 00:20:43,000
I was surprised

399
00:20:43,000 --> 00:20:46,000
at how good

400
00:20:46,000 --> 00:20:49,000
the basic data was.

401
00:20:49,000 --> 00:20:52,000
Some portals

402
00:20:52,000 --> 00:20:55,000
that are focused on Slovenia

403
00:20:55,000 --> 00:20:58,000
have corpuses.

404
00:20:58,000 --> 00:21:01,000
If someone wants to play with a ball,

405
00:21:01,000 --> 00:21:04,000
they don't start with a corpus.

406
00:21:04,000 --> 00:21:07,000
They start with the data

407
00:21:07,000 --> 00:21:10,000
on the will,

408
00:21:10,000 --> 00:21:13,000
and the portals are organized.

409
00:21:13,000 --> 00:21:16,000
I was surprised

410
00:21:16,000 --> 00:21:19,000
that those things exist.

411
00:21:19,000 --> 00:21:22,000
Maybe some comment on that.

412
00:21:22,000 --> 00:21:25,000
A lot of work has already been done,

413
00:21:25,000 --> 00:21:28,000
and it is also very personal.

414
00:21:28,000 --> 00:21:31,000
A lot of work on this topic has been done,

415
00:21:31,000 --> 00:21:34,000
but we should know

416
00:21:34,000 --> 00:21:37,000
that a lot of work has already been done.

417
00:21:37,000 --> 00:21:40,000
A few days ago,

418
00:21:40,000 --> 00:21:43,000
there were a lot of linguistic models.

419
00:21:43,000 --> 00:21:46,000
This work was done

420
00:21:46,000 --> 00:21:49,000
to digitize and preserve the language.

421
00:21:49,000 --> 00:21:52,000
Our goal is not only

422
00:21:52,000 --> 00:21:55,000
to digitize the language

423
00:21:55,000 --> 00:21:58,000
so that we can make technical solutions,

424
00:21:58,000 --> 00:22:01,000
but also to teach the language as a language.

425
00:22:01,000 --> 00:22:04,000
This is the language of Slavs.

426
00:22:04,000 --> 00:22:07,000
I think that a decade of research

427
00:22:07,000 --> 00:22:10,000
from different departments

428
00:22:10,000 --> 00:22:13,000
and universities in Slovenia

429
00:22:13,000 --> 00:22:16,000
was made in the 20th century.

430
00:22:19,000 --> 00:22:22,000
Unfortunately, when we talk about

431
00:22:22,000 --> 00:22:25,000
large linguistic models,

432
00:22:25,000 --> 00:22:28,000
the basis we have is simply too small.

433
00:22:28,000 --> 00:22:31,000
It does not help us.

434
00:22:31,000 --> 00:22:34,000
We have to decide

435
00:22:34,000 --> 00:22:37,000
whether to build a linguistic model

436
00:22:37,000 --> 00:22:40,000
or specialize it.

437
00:22:40,000 --> 00:22:43,000
Today, we can specialize

438
00:22:43,000 --> 00:22:46,000
a large linguistic model for a specific task,

439
00:22:46,000 --> 00:22:49,000
and we can do it well for the Slavs.

440
00:22:49,000 --> 00:22:52,000
But we do not have to do

441
00:22:52,000 --> 00:22:55,000
anything in the field of medicine.

442
00:22:55,000 --> 00:22:58,000
Until Slovenian medicine is included

443
00:22:58,000 --> 00:23:01,000
in large linguistic models,

444
00:23:01,000 --> 00:23:04,000
it will be even harder.

445
00:23:04,000 --> 00:23:07,000
We will not learn anything from it.

446
00:23:07,000 --> 00:23:10,000
Before we move on,

447
00:23:10,000 --> 00:23:13,000
I would like to ask you

448
00:23:13,000 --> 00:23:16,000
more about large models.

449
00:23:16,000 --> 00:23:19,000
Can you present to our audience

450
00:23:19,000 --> 00:23:22,000
a collection of very interesting

451
00:23:22,000 --> 00:23:25,000
and useful tools that you have developed

452
00:23:25,000 --> 00:23:28,000
in this segment?

453
00:23:28,000 --> 00:23:31,000
Yes, of course.

454
00:23:31,000 --> 00:23:34,000
The development of Slovenian digital environment

455
00:23:34,000 --> 00:23:37,000
was the first big project

456
00:23:37,000 --> 00:23:40,000
where we all got together

457
00:23:40,000 --> 00:23:43,000
in Slovenia.

458
00:23:43,000 --> 00:23:46,000
The Minister of Culture

459
00:23:46,000 --> 00:23:49,000
and I were co-financiers.

460
00:23:49,000 --> 00:23:52,000
We raised a lot of money.

461
00:23:52,000 --> 00:23:55,000
I think it was a four-million-euro project.

462
00:23:55,000 --> 00:23:58,000
The goal was to prepare such a trust.

463
00:23:58,000 --> 00:24:01,000
We also raised funds

464
00:24:01,000 --> 00:24:04,000
or bought the whole thing.

465
00:24:04,000 --> 00:24:07,000
We had money for the purchase.

466
00:24:07,000 --> 00:24:10,000
We also built solutions

467
00:24:10,000 --> 00:24:13,000
in the field of natural language processing

468
00:24:13,000 --> 00:24:16,000
and in the field of speech technologies.

469
00:24:16,000 --> 00:24:19,000
This means recognition of Slavic speech,

470
00:24:19,000 --> 00:24:22,000
translation of Slovak, English,

471
00:24:22,000 --> 00:24:25,000
English-Slovak and,

472
00:24:25,000 --> 00:24:28,000
which is probably also very important,

473
00:24:28,000 --> 00:24:31,000
we had to prepare a thousand-hour database

474
00:24:31,000 --> 00:24:34,000
of recordings of public speech,

475
00:24:34,000 --> 00:24:37,000
private speech, spoken speech,

476
00:24:37,000 --> 00:24:40,000
which can then be used

477
00:24:40,000 --> 00:24:43,000
for recognition.

478
00:24:43,000 --> 00:24:46,000
We did this in such a way

479
00:24:46,000 --> 00:24:49,000
that the database was also available

480
00:24:49,000 --> 00:24:52,000
for commercial use.

481
00:24:52,000 --> 00:24:55,000
Based on this database,

482
00:24:55,000 --> 00:24:58,000
we also created a publicly available

483
00:24:58,000 --> 00:25:01,000
recognition and translation database.

484
00:25:00,000 --> 00:25:03,000
In Slovenia, it had a 30% decline,

485
00:25:03,000 --> 00:25:06,000
but when we did it, it fell below 10%.

486
00:25:06,000 --> 00:25:09,000
I think it was 6% for some multiples,

487
00:25:09,000 --> 00:25:11,000
which is much better.

488
00:25:11,000 --> 00:25:14,000
And Prevajalnik was also taken to a certain area

489
00:25:14,000 --> 00:25:17,000
much better than Google,

490
00:25:17,000 --> 00:25:20,000
but not to a certain area.

491
00:25:20,000 --> 00:25:23,000
And this is also publicly available

492
00:25:23,000 --> 00:25:26,000
and companies are already using it.

493
00:25:26,000 --> 00:25:29,000
I think this project was very useful,

494
00:25:29,000 --> 00:25:32,000
but we may have made a mistake

495
00:25:32,000 --> 00:25:35,000
in the operational understanding

496
00:25:35,000 --> 00:25:38,000
of the order,

497
00:25:38,000 --> 00:25:41,000
because it is part of the resource

498
00:25:41,000 --> 00:25:44,000
of the Minister of Culture.

499
00:25:44,000 --> 00:25:47,000
It is very good that they financed it,

500
00:25:47,000 --> 00:25:50,000
but I think that on their side,

501
00:25:50,000 --> 00:25:53,000
they also admit a lack of technical understanding,

502
00:25:53,000 --> 00:25:56,000
and this part is perhaps a bit uncomfortable for us,

503
00:25:56,000 --> 00:25:59,000
because we have to,

504
00:25:59,000 --> 00:26:02,000
on various resources,

505
00:26:02,000 --> 00:26:05,000
as you know, we got the government again

506
00:26:05,000 --> 00:26:08,000
from the Minister of Digital Culture,

507
00:26:08,000 --> 00:26:11,000
and then we don't know who to talk to anymore,

508
00:26:11,000 --> 00:26:14,000
that such projects should not be stopped,

509
00:26:14,000 --> 00:26:17,000
because if they are stopped,

510
00:26:17,000 --> 00:26:20,000
we will remain at some point,

511
00:26:20,000 --> 00:26:23,000
but I think it is a very good thing

512
00:26:23,000 --> 00:26:26,000
that you did through this project

513
00:26:26,000 --> 00:26:29,000
Slovenian EU,

514
00:26:29,000 --> 00:26:32,000
that all these tools have a web interface

515
00:26:32,000 --> 00:26:35,000
and you can use them today,

516
00:26:35,000 --> 00:26:38,000
check them and try them,

517
00:26:38,000 --> 00:26:41,000
and use them today,

518
00:26:41,000 --> 00:26:44,000
it is very good that all the code is on GitHub

519
00:26:44,000 --> 00:26:47,000
and you can download it,

520
00:26:47,000 --> 00:26:50,000
because I saw that people today

521
00:26:50,000 --> 00:26:53,000
open poll requests,

522
00:26:53,000 --> 00:26:56,000
and today not only some research project

523
00:26:56,000 --> 00:26:59,000
would be done because of a research project,

524
00:26:59,000 --> 00:27:02,000
and then it is published in some academic archives,

525
00:27:02,000 --> 00:27:05,000
but it seems that there is an interest from a wider audience

526
00:27:05,000 --> 00:27:08,000
to help you with this,

527
00:27:08,000 --> 00:27:11,000
I have such a feeling.

528
00:27:11,000 --> 00:27:14,000
Hold on, I have to correct you a bit,

529
00:27:14,000 --> 00:27:17,000
when we talk about misunderstanding,

530
00:27:17,000 --> 00:27:20,000
a model is a model,

531
00:27:20,000 --> 00:27:23,000
it is not a solver by itself.

532
00:27:23,000 --> 00:27:26,000
If we make a model for the recognition of Slovenia,

533
00:27:26,000 --> 00:27:29,000
someone expects to have subtitles on the TV

534
00:27:29,000 --> 00:27:32,000
that are generated automatically in real time.

535
00:27:32,000 --> 00:27:35,000
This means a complicated use of such a model,

536
00:27:35,000 --> 00:27:38,000
or a model that is learned from it.

537
00:27:38,000 --> 00:27:41,000
This problem is not a problem

538
00:27:41,000 --> 00:27:44,000
that was big in the past.

539
00:27:44,000 --> 00:27:47,000
We made a demonstration

540
00:27:47,000 --> 00:27:50,000
on slovencina.eu portal,

541
00:27:50,000 --> 00:27:53,000
where you can submit a speech

542
00:27:53,000 --> 00:27:56,000
and get a transcript,

543
00:27:56,000 --> 00:27:59,000
but it was limited to 5 minutes.

544
00:27:59,000 --> 00:28:02,000
Why me? Because we don't have such a hardware

545
00:28:02,000 --> 00:28:05,000
for the whole of Slovenia to fulfill it.

546
00:28:05,000 --> 00:28:08,000
You have a model for this,

547
00:28:08,000 --> 00:28:11,000
but you don't have the whole culture.

548
00:28:11,000 --> 00:28:14,000
We gave so much money,

549
00:28:14,000 --> 00:28:17,000
we have to understand that the application

550
00:28:17,000 --> 00:28:20,000
is not the same as the model.

551
00:28:20,000 --> 00:28:23,000
The model allows you to develop

552
00:28:23,000 --> 00:28:26,000
very different solutions,

553
00:28:26,000 --> 00:28:29,000
but it is not the final solution.

554
00:28:29,000 --> 00:28:32,000
In the final phase,

555
00:28:32,000 --> 00:28:35,000
I think it is a meaningful measure

556
00:28:35,000 --> 00:28:38,000
between what the research community does,

557
00:28:38,000 --> 00:28:41,000
which is financed by the public,

558
00:28:41,000 --> 00:28:44,000
and what the private sector does.

559
00:28:44,000 --> 00:28:47,000
The private sector in Slovenia will not invest

560
00:28:47,000 --> 00:28:50,000
in the preparation of things that are needed

561
00:28:50,000 --> 00:28:53,000
for Slovenian,

562
00:28:53,000 --> 00:28:56,000
because it is too expensive.

563
00:28:56,000 --> 00:28:59,000
If you want to make a synthesizer for Slovenia,

564
00:28:59,000 --> 00:29:02,000
you need a professional speaker

565
00:29:02,000 --> 00:29:05,000
who will record it in a professional studio

566
00:29:05,000 --> 00:29:08,000
for 25 to 50 hours.

567
00:29:08,000 --> 00:29:11,000
Then you have to buy the rights,

568
00:29:11,000 --> 00:29:14,000
which are quite complicated,

569
00:29:14,000 --> 00:29:17,000
because he will give his voice,

570
00:29:17,000 --> 00:29:20,000
and the voice will appear.

571
00:29:20,000 --> 00:29:23,000
With the big ones, you can buy a synthesizer

572
00:29:23,000 --> 00:29:26,000
for 1 million zl for $20.

573
00:29:26,000 --> 00:29:29,000
Who will do this in Slovenia?

574
00:29:29,000 --> 00:29:32,000
To some extent, the state must drive

575
00:29:32,000 --> 00:29:35,000
if it wants to develop solutions

576
00:29:35,000 --> 00:29:38,000
that can have much more added value

577
00:29:38,000 --> 00:29:41,000
than this basis.

578
00:29:41,000 --> 00:29:44,000
Will you go next?

579
00:29:44,000 --> 00:29:47,000
I think it is fascinating.

580
00:29:47,000 --> 00:29:50,000
I am fascinated.

581
00:29:50,000 --> 00:29:53,000
For the listeners,

582
00:29:53,000 --> 00:29:56,000
it started to rain heavily in Zuni.

583
00:29:56,000 --> 00:29:59,000
Otok will talk about our supporters.

584
00:29:59,000 --> 00:30:02,000
We are already at the same moment.

585
00:30:02,000 --> 00:30:05,000
A little EPP to interrupt.

586
00:30:05,000 --> 00:30:08,000
In Grodje, it is possible because of our supporters

587
00:30:08,000 --> 00:30:11,000
via Patreon,

588
00:30:11,000 --> 00:30:14,000
who support us with monthly donations

589
00:30:14,000 --> 00:30:17,000
and get exclusive access

590
00:30:17,000 --> 00:30:20,000
to all the videos before the game is released.

591
00:30:20,000 --> 00:30:23,000
In Grodje, there are also supporters

592
00:30:23,000 --> 00:30:26,000
such as Tripodetia, Humanfrog, Trifas and Kaldi,

593
00:30:26,000 --> 00:30:29,000
who are very grateful to support us

594
00:30:29,000 --> 00:30:32,000
and allow us to work on this level.

595
00:30:32,000 --> 00:30:35,000
You said like and subscribe.

596
00:30:35,000 --> 00:30:38,000
Yes, like and subscribe,

597
00:30:38,000 --> 00:30:41,000
and tell your colleagues if they don't know

598
00:30:41,000 --> 00:30:44,000
to listen to one episode,

599
00:30:44,000 --> 00:30:47,000
and if it is OK, to listen to another.

600
00:30:47,000 --> 00:30:50,000
OK, this is an EPP. Thank you.

601
00:30:50,000 --> 00:30:53,000
Let's move on to the language models.

602
00:30:53,000 --> 00:30:56,000
We realized that you need a lot of volume,

603
00:30:56,000 --> 00:30:59,000
we realized that the legislation is interesting,

604
00:30:59,000 --> 00:31:02,000
we realized that you need filtering,

605
00:31:02,000 --> 00:31:05,000
you need to pay attention to what goes inside.

606
00:31:05,000 --> 00:31:08,000
We realized that the data is added approximately.

607
00:31:08,000 --> 00:31:11,000
Now we are more interested in the technological aspect.

608
00:31:11,000 --> 00:31:14,000
How do you get those ETL pipelines

609
00:31:14,000 --> 00:31:17,000
for that volume of data?

610
00:31:17,000 --> 00:31:20,000
This is a Java data conference.

611
00:31:20,000 --> 00:31:23,000
Can you tell us more about

612
00:31:23,000 --> 00:31:26,000
how much energy and resources

613
00:31:26,000 --> 00:31:29,000
do you need to invest in preparing the technology

614
00:31:29,000 --> 00:31:32,000
so that we can train these models?

615
00:31:32,000 --> 00:31:35,000
I can try.

616
00:31:35,000 --> 00:31:38,000
The last project we are developing is called Povejmo.

617
00:31:38,000 --> 00:31:41,000
In this project, we have to create

618
00:31:41,000 --> 00:31:44,000
a smaller Slovenian model

619
00:31:44,000 --> 00:31:47,000
that will have 1 billion parameters

620
00:31:47,000 --> 00:31:50,000
and then 10 billion.

621
00:31:50,000 --> 00:31:53,000
We are working on this smaller model

622
00:31:53,000 --> 00:31:56,000
to be able to make phones and so on.

623
00:31:56,000 --> 00:31:59,000
We don't start from scratch.

624
00:31:59,000 --> 00:32:02,000
We have taken other similar models

625
00:32:02,000 --> 00:32:05,000
and we are learning them in the future.

626
00:32:05,000 --> 00:32:08,000
The techniques are very different.

627
00:32:08,000 --> 00:32:11,000
I have already mentioned tokenization.

628
00:32:11,000 --> 00:32:14,000
Many of you who are technologists

629
00:32:14,000 --> 00:32:17,000
probably understand this.

630
00:32:17,000 --> 00:32:20,000
If we start from a different model,

631
00:32:20,000 --> 00:32:23,000
we can protect their vocabulary.

632
00:32:23,000 --> 00:32:26,000
But this will not be ideal

633
00:32:26,000 --> 00:32:29,000
because Slovenian is a different language

634
00:32:29,000 --> 00:32:32,000
and has a different vocabulary.

635
00:32:32,000 --> 00:32:35,000
There are probably some bones

636
00:32:35,000 --> 00:32:38,000
that are similar in meaning,

637
00:32:38,000 --> 00:32:41,000
so we have to create a new vocabulary.

638
00:32:41,000 --> 00:32:44,000
We have already done this,

639
00:32:44,000 --> 00:32:47,000
but we don't want to lose everything

640
00:32:47,000 --> 00:32:50,000
we have learned in the basic model.

641
00:32:50,000 --> 00:32:53,000
There are some techniques

642
00:32:53,000 --> 00:32:56,000
that we transfer this knowledge

643
00:32:56,000 --> 00:32:59,000
from the first model to our beginner

644
00:32:59,000 --> 00:33:02,000
and then we learn it with our words.

645
00:33:02,000 --> 00:33:05,000
As far as building a building is concerned,

646
00:33:05,000 --> 00:33:08,000
it is very different.

647
00:33:08,000 --> 00:33:11,000
What we are trying to professionalize

648
00:33:11,000 --> 00:33:14,000
is from this point onward

649
00:33:14,000 --> 00:33:17,000
when it comes to the faculty.

650
00:33:17,000 --> 00:33:20,000
This means intensive meetings

651
00:33:20,000 --> 00:33:23,000
that are intensively monitored.

652
00:33:23,000 --> 00:33:26,000
It is clear that the data is not lost.

653
00:33:26,000 --> 00:33:29,000
The volume is large,

654
00:33:29,000 --> 00:33:32,000
but the quantity in terabytes

655
00:33:32,000 --> 00:33:35,000
is not that big.

656
00:33:35,000 --> 00:33:38,000
It is difficult to handle this.

657
00:33:38,000 --> 00:33:41,000
Then there are the resources,

658
00:33:41,000 --> 00:33:44,000
the computing tools

659
00:33:44,000 --> 00:33:47,000
that we need for our work.

660
00:33:47,000 --> 00:33:50,000
We are targeting VEGA.

661
00:33:50,000 --> 00:33:53,000
VEGA is a well-known HPC

662
00:33:53,000 --> 00:33:56,000
that is installed in Maribor.

663
00:33:56,000 --> 00:33:59,000
For those who understand GPU technology,

664
00:33:59,000 --> 00:34:02,000
it has 64 cores and 4 A100 cards.

665
00:34:02,000 --> 00:34:05,000
This is a bit outdated technology,

666
00:34:05,000 --> 00:34:08,000
but it is enough for our size

667
00:34:08,000 --> 00:34:11,000
and our amount of data.

668
00:34:11,000 --> 00:34:14,000
We have 12 H100 cards at our faculty

669
00:34:14,000 --> 00:34:17,000
that are already better.

670
00:34:17,000 --> 00:34:20,000
We have installed two new data centers

671
00:34:20,000 --> 00:34:23,000
in the process.

672
00:34:23,000 --> 00:34:26,000
If NVIDIA really gave us GB200,

673
00:34:26,000 --> 00:34:29,000
it would not be in 5 flops

674
00:34:29,000 --> 00:34:32,000
with VEGA.

675
00:34:32,000 --> 00:34:35,000
It would be in a water-cooled container

676
00:34:35,000 --> 00:34:38,000
on the faculty roof.

677
00:34:38,000 --> 00:34:41,000
It would pay for electricity,

678
00:34:41,000 --> 00:34:44,000
which we have not changed yet.

679
00:34:44,000 --> 00:34:47,000
I think we still have half a year

680
00:34:47,000 --> 00:34:50,000
to finish the installation.

681
00:34:50,000 --> 00:34:53,000
Did I answer your question?

682
00:34:53,000 --> 00:34:56,000
No, I told AndraÅ¾ that I would not

683
00:34:56,000 --> 00:34:59,000
answer the hardware question.

684
00:34:59,000 --> 00:35:02,000
No, no.

685
00:35:02,000 --> 00:35:05,000
People are often critical.

686
00:35:05,000 --> 00:35:08,000
They like to complain.

687
00:35:08,000 --> 00:35:11,000
We were like, OK, why do we need

688
00:35:11,000 --> 00:35:14,000
this HPC?

689
00:35:14,000 --> 00:35:17,000
Do you think that with LLMI

690
00:35:17,000 --> 00:35:20,000
we will be a bit more popular?

691
00:35:20,000 --> 00:35:23,000
We need more preparation.

692
00:35:23,000 --> 00:35:26,000
Of course, now we need GPUs

693
00:35:26,000 --> 00:35:29,000
or something like that.

694
00:35:29,000 --> 00:35:32,000
I am in a dilemma.

695
00:35:32,000 --> 00:35:35,000
I personally think that we have

696
00:35:35,000 --> 00:35:38,000
more than we need.

697
00:35:38,000 --> 00:35:41,000
In my role as a professor at the faculty,

698
00:35:41,000 --> 00:35:44,000
I think it is still too little.

699
00:35:44,000 --> 00:35:47,000
We do not have to wait for others.

700
00:35:47,000 --> 00:35:50,000
We want to work with the best.

701
00:35:50,000 --> 00:35:53,000
We have other skills when it comes to language.

702
00:35:53,000 --> 00:35:56,000
Then we have to go the same way.

703
00:35:56,000 --> 00:35:59,000
No one else chooses.

704
00:35:59,000 --> 00:36:02,000
The installation, what it brings us,

705
00:36:02,000 --> 00:36:05,000
is a different question.

706
00:36:05,000 --> 00:36:08,000
It has its own philosophical dimension.

707
00:36:08,000 --> 00:36:11,000
I will not go into that.

708
00:36:11,000 --> 00:36:14,000
That is the same question.

709
00:36:14,000 --> 00:36:17,000
OK, we are at a Java conference.

710
00:36:17,000 --> 00:36:20,000
Is it written in Python?

711
00:36:20,000 --> 00:36:23,000
No?

712
00:36:23,000 --> 00:36:26,000
No, it is mostly in Python.

713
00:36:26,000 --> 00:36:29,000
Python is simply a data science language.

714
00:36:29,000 --> 00:36:32,000
You do not have it in Java.

715
00:36:32,000 --> 00:36:35,000
There is no mutual love between you and Python.

716
00:36:35,000 --> 00:36:38,000
That is known.

717
00:36:38,000 --> 00:36:41,000
We have to watch how we arrange programmers.

718
00:36:41,000 --> 00:36:44,000
It is true that Python is meant for those things.

719
00:36:44,000 --> 00:36:47,000
It is not written for enterprise software,

720
00:36:47,000 --> 00:36:50,000
but it is meant for those things.

721
00:36:50,000 --> 00:36:53,000
Most of the tools we use are written in Python.

722
00:36:53,000 --> 00:36:56,000
But Python as a glue,

723
00:36:56,000 --> 00:36:59,000
we probably have a C in the back,

724
00:36:59,000 --> 00:37:02,000
for ultra-optimized tasks.

725
00:37:02,000 --> 00:37:05,000
Yes, of course.

726
00:37:05,000 --> 00:37:08,000
Some libraries that Python uses

727
00:37:08,000 --> 00:37:11,000
make a military around a very fast core.

728
00:37:11,000 --> 00:37:14,000
That is done.

729
00:37:14,000 --> 00:37:17,000
If we go into that parallelism

730
00:37:17,000 --> 00:37:20,000
in any architecture,

731
00:37:20,000 --> 00:37:23,000
of course, it is not Python.

732
00:37:23,000 --> 00:37:26,000
Do you have to rush a lot?

733
00:37:26,000 --> 00:37:29,000
If you have VEGO,

734
00:37:29,000 --> 00:37:32,000
do you have to rush?

735
00:37:32,000 --> 00:37:35,000
Is it such a big effort?

736
00:37:35,000 --> 00:37:38,000
No, it is not trivial.

737
00:37:38,000 --> 00:37:41,000
Today, most young people

738
00:37:41,000 --> 00:37:44,000
already know a lot about Docker and those things.

739
00:37:44,000 --> 00:37:47,000
But on the server,

740
00:37:47,000 --> 00:37:50,000
as a job manager in such large systems,

741
00:37:50,000 --> 00:37:53,000
you hear it for the first time.

742
00:37:53,000 --> 00:37:56,000
Or on Singularity and similar things

743
00:37:56,000 --> 00:37:59,000
that are used on such HPC systems.

744
00:37:59,000 --> 00:38:02,000
But the learning curve is not that long.

745
00:38:02,000 --> 00:38:05,000
It is necessary to write concise scripts

746
00:38:05,000 --> 00:38:08,000
that give a job in order and can be prepared.

747
00:38:08,000 --> 00:38:11,000
We have to do everything in such a way

748
00:38:11,000 --> 00:38:14,000
that there is no interaction,

749
00:38:14,000 --> 00:38:17,000
that it can flow smoothly,

750
00:38:17,000 --> 00:38:20,000
because the models we teach

751
00:38:20,000 --> 00:38:23,000
do not have a guarantee that the task will be completed.

752
00:38:23,000 --> 00:38:26,000
It can fall at any step.

753
00:38:26,000 --> 00:38:29,000
It has to be done in such a way

754
00:38:29,000 --> 00:38:32,000
that it is not a bad art.

755
00:38:32,000 --> 00:38:35,000
But if you give me a person

756
00:38:35,000 --> 00:38:38,000
who has never heard of Linux

757
00:38:38,000 --> 00:38:41,000
and would like to be a data scientist

758
00:38:41,000 --> 00:38:44,000
and run it on HPC machines,

759
00:38:44,000 --> 00:38:47,000
it will not work.

760
00:38:47,000 --> 00:38:50,000
I don't even know where to start.

761
00:38:50,000 --> 00:38:53,000
Can you give an illustration

762
00:38:53,000 --> 00:38:56,000
of how long these jobs last on supercomputers?

763
00:38:56,000 --> 00:38:59,000
Of course.

764
00:38:59,000 --> 00:39:02,000
I will give a few examples,

765
00:39:02,000 --> 00:39:05,000
and then you can imagine.

766
00:39:05,000 --> 00:39:08,000
For fine-tuning on LLM,

767
00:39:08,000 --> 00:39:11,000
even if we take a LAM,

768
00:39:11,000 --> 00:39:14,000
a larger one, let's say 10 billion,

769
00:39:14,000 --> 00:39:17,000
if we work only on one era,

770
00:39:17,000 --> 00:39:20,000
it can be done in a few days.

771
00:39:20,000 --> 00:39:23,000
Because it is only fine-tuning.

772
00:39:23,000 --> 00:39:26,000
Techniques are being developed,

773
00:39:26,000 --> 00:39:29,000
such as Solora,

774
00:39:29,000 --> 00:39:32,000
which we can use to make

775
00:39:32,000 --> 00:39:35,000
fine-tuning of the model faster.

776
00:39:35,000 --> 00:39:38,000
This can be done in a week

777
00:39:38,000 --> 00:39:41,000
on extremely powerful machines.

778
00:39:41,000 --> 00:39:44,000
I would say the same for synthesis

779
00:39:44,000 --> 00:39:47,000
and recognition.

780
00:39:47,000 --> 00:39:50,000
I think we have a week to two weeks.

781
00:39:50,000 --> 00:39:53,000
When we talk about basic language models,

782
00:39:53,000 --> 00:39:56,000
I think we don't even have enough hardware.

783
00:39:56,000 --> 00:39:59,000
If we wanted as many as they do

784
00:39:59,000 --> 00:40:02,000
for big languages,

785
00:40:02,000 --> 00:40:05,000
we don't have enough of that.

786
00:40:05,000 --> 00:40:08,000
When it comes to these

787
00:40:08,000 --> 00:40:11,000
open-source language models,

788
00:40:11,000 --> 00:40:14,000
what exactly does this open-source mean?

789
00:40:14,000 --> 00:40:17,000
We are talking about open use,

790
00:40:17,000 --> 00:40:20,000
what do you see in here?

791
00:40:20,000 --> 00:40:23,000
Is it really open?

792
00:40:23,000 --> 00:40:26,000
And where is the secret sauce?

793
00:40:35,000 --> 00:40:38,000
In models, open source means

794
00:40:38,000 --> 00:40:41,000
mostly that you have a model

795
00:40:41,000 --> 00:40:44,000
with the right hardware.

796
00:40:44,000 --> 00:40:47,000
How was this model made?

797
00:40:47,000 --> 00:40:50,000
What was included?

798
00:40:50,000 --> 00:40:53,000
It is not necessary to know.

799
00:40:53,000 --> 00:40:56,000
But you have a model with all its hardware.

800
00:40:56,000 --> 00:40:59,000
You can take it and build from it.

801
00:40:59,000 --> 00:41:02,000
This is an open-source model of this type.

802
00:41:02,000 --> 00:41:05,000
Commercial ones are not so open.

803
00:41:05,000 --> 00:41:08,000
If you want to take GPT-3,

804
00:41:08,000 --> 00:41:11,000
you can't.

805
00:41:11,000 --> 00:41:14,000
Some people do more steps,

806
00:41:14,000 --> 00:41:17,000
others do less.

807
00:41:17,000 --> 00:41:20,000
I don't think this is fair.

808
00:41:20,000 --> 00:41:23,000
Do you think that this will be the first part,

809
00:41:23,000 --> 00:41:26,000
that we will want to have a whole

810
00:41:26,000 --> 00:41:29,000
regulative system that will want to know

811
00:41:29,000 --> 00:41:32,000
the accuracy of the data,

812
00:41:32,000 --> 00:41:35,000
that we will be able to have better written

813
00:41:35,000 --> 00:41:38,000
and defined training,

814
00:41:38,000 --> 00:41:41,000
or is it just technical and inexplicable?

815
00:41:41,000 --> 00:41:44,000
I don't know where it will go,

816
00:41:44,000 --> 00:41:47,000
because we are trying to have

817
00:41:47,000 --> 00:41:50,000
more and more ethics in the field of

818
00:41:50,000 --> 00:41:53,000
the use of artificial intelligence,

819
00:41:53,000 --> 00:41:56,000
and then a lot of different interests

820
00:41:56,000 --> 00:41:59,000
arise.

821
00:41:59,000 --> 00:42:02,000
I can say that

822
00:42:02,000 --> 00:42:05,000
generative models

823
00:42:05,000 --> 00:42:08,000
can't serve you data

824
00:42:08,000 --> 00:42:11,000
from which a certain answer has come,

825
00:42:11,000 --> 00:42:14,000
unless they give you all the answers

826
00:42:14,000 --> 00:42:17,000
that you wanted.

827
00:42:17,000 --> 00:42:20,000
This is a statistical model.

828
00:42:20,000 --> 00:42:23,000
The most likely answer is 1.5.

829
00:42:23,000 --> 00:42:26,000
I think we will deal much more with

830
00:42:26,000 --> 00:42:29,000
informing people that these answers

831
00:42:29,000 --> 00:42:32,000
are not necessarily correct.

832
00:42:33,000 --> 00:42:36,000
We will learn where this technology

833
00:42:36,000 --> 00:42:39,000
is really useful,

834
00:42:39,000 --> 00:42:42,000
and where it is still dangerous.

835
00:42:42,000 --> 00:42:45,000
This distinction will have to be made.

836
00:42:45,000 --> 00:42:48,000
At the University, we made a step

837
00:42:48,000 --> 00:42:51,000
not to order, but to recommend

838
00:42:51,000 --> 00:42:54,000
that an ethical commission be set up,

839
00:42:54,000 --> 00:42:57,000
which will check where it is going.

840
00:42:57,000 --> 00:43:00,000
At the end of the day,

841
00:43:00,000 --> 00:43:03,000
this is a good intention,

842
00:43:03,000 --> 00:43:06,000
but the result is harmful.

843
00:43:06,000 --> 00:43:09,000
As I said, we can't expect

844
00:43:09,000 --> 00:43:12,000
to be serviced by trusts.

845
00:43:12,000 --> 00:43:15,000
If you go and ask

846
00:43:15,000 --> 00:43:18,000
what the trust will write to you,

847
00:43:18,000 --> 00:43:21,000
it is because trust is not only

848
00:43:21,000 --> 00:43:24,000
a linguistic model.

849
00:43:24,000 --> 00:43:27,000
It will probably also write to you

850
00:43:27,000 --> 00:43:30,000
with the same trust that it took

851
00:43:30,000 --> 00:43:33,000
to generate it.

852
00:43:33,000 --> 00:43:36,000
It can't be the same with the generation itself.

853
00:43:36,000 --> 00:43:39,000
I gave him a task for a new project,

854
00:43:39,000 --> 00:43:42,000
because I know he will write it very well.

855
00:43:42,000 --> 00:43:45,000
I told him to write me an integration

856
00:43:45,000 --> 00:43:48,000
between me, a linguist,

857
00:43:48,000 --> 00:43:51,000
a state-of-the-art, a challenge,

858
00:43:51,000 --> 00:43:54,000
a work package.

859
00:43:54,000 --> 00:43:57,000
He said yes, but he also added

860
00:43:57,000 --> 00:44:00,000
references to Motivation.

861
00:44:00,000 --> 00:44:03,000
I knew I wouldn't get those references,

862
00:44:03,000 --> 00:44:06,000
but I was interested in what I would get.

863
00:44:06,000 --> 00:44:09,000
I tried three of them.

864
00:44:09,000 --> 00:44:12,000
One of them I already invented.

865
00:44:12,000 --> 00:44:15,000
They were in Serbian.

866
00:44:15,000 --> 00:44:18,000
It would have been great if someone

867
00:44:18,000 --> 00:44:21,000
really wrote it.

868
00:44:21,000 --> 00:44:24,000
But the authors were serious,

869
00:44:24,000 --> 00:44:27,000
and the article could have been

870
00:44:27,000 --> 00:44:30,000
a different word.

871
00:44:30,000 --> 00:44:33,000
And it wasn't Tewik who wrote it,

872
00:44:33,000 --> 00:44:36,000
but it was most likely that

873
00:44:36,000 --> 00:44:39,000
the article would have been sent

874
00:44:39,000 --> 00:44:42,000
and it would have been generated.

875
00:44:42,000 --> 00:44:45,000
I think that people don't understand

876
00:44:45,000 --> 00:44:48,000
what's going on behind the scenes.

877
00:44:48,000 --> 00:44:51,000
That's why it's very important

878
00:44:51,000 --> 00:44:54,000
to know what we are using

879
00:44:54,000 --> 00:44:57,000
and what are the potential risks.

880
00:44:57,000 --> 00:45:00,000
That's why I often say

881
00:45:00,000 --> 00:45:03,000
that the other sector should get

882
00:45:03,000 --> 00:45:06,000
more money to speed up its pace.

883
00:45:06,000 --> 00:45:09,000
We make decisions too quickly.

884
00:45:09,000 --> 00:45:12,000
They come out too quickly.

885
00:45:12,000 --> 00:45:15,000
If the market is not ready

886
00:45:15,000 --> 00:45:18,000
for them, the society is not prepared

887
00:45:18,000 --> 00:45:21,000
for them.

888
00:45:21,000 --> 00:45:24,000
We are not prepared for them

889
00:45:24,000 --> 00:45:27,000
in terms of regulation or ethics.

890
00:45:27,000 --> 00:45:30,000
An example I often give to my students

891
00:45:30,000 --> 00:45:33,000
is the autonomous vehicle.

892
00:45:33,000 --> 00:45:36,000
If they find themselves in a situation

893
00:45:36,000 --> 00:45:39,000
where there are only two choices,

894
00:45:39,000 --> 00:45:42,000
a child or a grandmother,

895
00:45:42,000 --> 00:45:45,000
they have someone coded into their model.

896
00:45:45,000 --> 00:45:48,000
Isn't that morally disputable?

897
00:45:48,000 --> 00:45:51,000
But such decisions already exist

898
00:45:51,000 --> 00:45:54,000
and we are trying to introduce them.

899
00:45:54,000 --> 00:45:57,000
That's why I say that we are in a time

900
00:45:57,000 --> 00:46:00,000
where technology is great

901
00:46:00,000 --> 00:46:03,000
and we find it fascinating

902
00:46:03,000 --> 00:46:06,000
when we can do things that move humanity.

903
00:46:06,000 --> 00:46:09,000
But I don't know if we are fully prepared

904
00:46:09,000 --> 00:46:12,000
to introduce them in a smart way.

905
00:46:12,000 --> 00:46:15,000
I think we need to think about that.

906
00:46:22,000 --> 00:46:25,000
A year ago, we had an episode

907
00:46:25,000 --> 00:46:28,000
where we talked about LLM and AI.

908
00:46:32,000 --> 00:46:35,000
And now that we are talking about it,

909
00:46:35,000 --> 00:46:38,000
in six months, I think they are moving

910
00:46:38,000 --> 00:46:41,000
forward very quickly.

911
00:46:41,000 --> 00:46:44,000
Some things seem self-evident to us.

912
00:46:44,000 --> 00:46:47,000
The strategic importance of Slovenia

913
00:46:47,000 --> 00:46:50,000
will rise.

914
00:46:50,000 --> 00:46:53,000
I think we put forward this month

915
00:46:53,000 --> 00:46:56,000
that this is important, that we need to deal with it,

916
00:46:56,000 --> 00:46:59,000
that we need to be proactive,

917
00:46:59,000 --> 00:47:02,000
that this is not a joke.

918
00:47:02,000 --> 00:47:05,000
But these technological LLM things...

919
00:47:05,000 --> 00:47:08,000
I won't say where we will be in three months,

920
00:47:08,000 --> 00:47:11,000
but we are working on it,

921
00:47:11,000 --> 00:47:14,000
and maybe in three months we will say

922
00:47:14,000 --> 00:47:17,000
that this is normal.

923
00:47:17,000 --> 00:47:20,000
I think three beers would be enough

924
00:47:20,000 --> 00:47:23,000
to answer that.

925
00:47:23,000 --> 00:47:26,000
To predict the future of AI

926
00:47:26,000 --> 00:47:29,000
is more like going to a bar,

927
00:47:29,000 --> 00:47:32,000
where a person forgives and philosophizes.

928
00:47:33,000 --> 00:47:36,000
Personally, I don't feel that I am a guru

929
00:47:36,000 --> 00:47:39,000
in this area,

930
00:47:39,000 --> 00:47:42,000
but from everything I understand

931
00:47:42,000 --> 00:47:45,000
I think that at the moment

932
00:47:45,000 --> 00:47:48,000
we have a lot of solutions

933
00:47:48,000 --> 00:47:51,000
that use very good models,

934
00:47:51,000 --> 00:47:54,000
taught on a huge amount of data.

935
00:47:54,000 --> 00:47:57,000
But for that we need a lot of time

936
00:47:57,000 --> 00:48:00,000
to learn them and then use them.

937
00:48:01,000 --> 00:48:04,000
We learn them, then we make inferences,

938
00:48:04,000 --> 00:48:07,000
then we learn, and so on.

939
00:48:07,000 --> 00:48:10,000
And this cycle is driven

940
00:48:10,000 --> 00:48:13,000
by a very fast development of hardware.

941
00:48:13,000 --> 00:48:16,000
It can be reduced in such a way

942
00:48:16,000 --> 00:48:19,000
that in some areas

943
00:48:19,000 --> 00:48:22,000
it can start in real time.

944
00:48:22,000 --> 00:48:25,000
This is my additional step forward.

945
00:48:31,000 --> 00:48:34,000
A few years ago we installed

946
00:48:34,000 --> 00:48:37,000
VEGO in Maribor,

947
00:48:37,000 --> 00:48:40,000
which is so big and spacious,

948
00:48:40,000 --> 00:48:43,000
and now it is in one of the Omars.

949
00:48:43,000 --> 00:48:46,000
This shows that there is still

950
00:48:46,000 --> 00:48:49,000
a lot of space here,

951
00:48:49,000 --> 00:48:52,000
and when it is possible

952
00:48:52,000 --> 00:48:55,000
to learn the system in real time,

953
00:48:55,000 --> 00:48:58,000
I think that will be a big step forward.

954
00:48:59,000 --> 00:49:02,000
One more question related to this.

955
00:49:02,000 --> 00:49:05,000
We are talking about 1 billion tokens,

956
00:49:05,000 --> 00:49:08,000
10 billion tokens,

957
00:49:08,000 --> 00:49:11,000
LAMA has 70 billion,

958
00:49:11,000 --> 00:49:14,000
will we get to the point

959
00:49:14,000 --> 00:49:17,000
where the addition of these tokens

960
00:49:17,000 --> 00:49:20,000
has a cap?

961
00:49:20,000 --> 00:49:23,000
Do we need to add 1000 billion?

962
00:49:23,000 --> 00:49:26,000
Will this remain somewhere

963
00:49:26,000 --> 00:49:29,000
between 1 billion and 1 billion?

964
00:49:29,000 --> 00:49:32,000
Will we be able to implement

965
00:49:32,000 --> 00:49:35,000
more algorithms,

966
00:49:35,000 --> 00:49:38,000
or a fundamental layer?

967
00:49:38,000 --> 00:49:41,000
Do you see this as a trend?

968
00:49:41,000 --> 00:49:44,000
Is there a top line?

969
00:49:44,000 --> 00:49:47,000
There is almost always a top line.

970
00:49:47,000 --> 00:49:50,000
When you ask the question,

971
00:49:50,000 --> 00:49:53,000
the top line is business-oriented,

972
00:49:54,000 --> 00:49:57,000
but I also think that

973
00:49:57,000 --> 00:50:00,000
business-oriented and technical

974
00:50:00,000 --> 00:50:03,000
is the top line.

975
00:50:00,000 --> 00:50:09,000
What we need is to learn all the hidden and well-known meanings of the language.

976
00:50:09,000 --> 00:50:16,000
I personally think that there is a limit that we do not need to go beyond,

977
00:50:16,000 --> 00:50:21,000
because we will somehow eat up all the features.

978
00:50:21,000 --> 00:50:27,000
But the language is very large, the way of expression is very diverse,

979
00:50:27,000 --> 00:50:34,000
the personality is also huge, and this limit is potentially very high.

980
00:50:34,000 --> 00:50:40,000
Andras, I will not say that it is time for a beer, maybe it is time for a snack,

981
00:50:40,000 --> 00:50:43,000
so you will have one last question.

982
00:50:43,000 --> 00:50:46,000
Yes.

983
00:50:46,000 --> 00:50:51,000
I hope to get one question from the audience.

984
00:50:51,000 --> 00:50:52,000
I can try.

985
00:50:52,000 --> 00:50:56,000
Let's go a little out of the comfort zone.

986
00:50:56,000 --> 00:51:02,000
Who will raise their hand? Who hopes?

987
00:51:02,000 --> 00:51:06,000
Exclusive opportunity.

988
00:51:06,000 --> 00:51:09,000
Among the projects that are more technically advanced,

989
00:51:09,000 --> 00:51:15,000
the most important question is more about the project that would inform the public

990
00:51:15,000 --> 00:51:20,000
about the logical understanding of these models,

991
00:51:20,000 --> 00:51:24,000
or what exactly you said, that there would be a problem with some applications,

992
00:51:24,000 --> 00:51:26,000
that they would not be able to do something.

993
00:51:26,000 --> 00:51:29,000
Now we have a slightly biased Slovenian,

994
00:51:29,000 --> 00:51:32,000
because we have seen a new type of AI in the world,

995
00:51:32,000 --> 00:51:34,000
and that's it.

996
00:51:34,000 --> 00:51:35,000
Yes.

997
00:51:51,000 --> 00:51:55,000
Now I will have the question again to go to Eder.

998
00:51:55,000 --> 00:52:01,000
Is there a project that would educate the public about these technologies,

999
00:52:01,000 --> 00:52:07,000
so that the wider public would be more prepared for these technologies?

1000
00:52:07,000 --> 00:52:11,000
I will be very short, so that the audience can hear.

1001
00:52:11,000 --> 00:52:14,000
Yes, Marcin, everything is happening.

1002
00:52:14,000 --> 00:52:19,000
I have already seen some of the manuscripts,

1003
00:52:19,000 --> 00:52:25,000
where they dealt with large language models from a non-technical aspect.

1004
00:52:25,000 --> 00:52:29,000
So, I expect that there are also such views.

1005
00:52:30,000 --> 00:52:38,000
At our faculty, we try to include interdisciplinary groups,

1006
00:52:38,000 --> 00:52:46,000
because we are technicians, and we can explain how it works.

1007
00:52:46,000 --> 00:52:52,000
But what impact does it have on society, on young people, on I don't know what.

1008
00:52:52,000 --> 00:52:57,000
This is not our area, which we could really understand,

1009
00:52:57,000 --> 00:53:01,000
because we do not deal with it, and we are looking for others.

1010
00:53:01,000 --> 00:53:05,000
And when I mentioned the Ethical Commission at the University,

1011
00:53:05,000 --> 00:53:07,000
it was set up that way.

1012
00:53:07,000 --> 00:53:10,000
But in the beginning it was without us, without technology.

1013
00:53:10,000 --> 00:53:15,000
We are now on the same level, because now we know what these things are,

1014
00:53:15,000 --> 00:53:19,000
because we have seen many times that many people are biased,

1015
00:53:19,000 --> 00:53:22,000
and they do not know what it is.

1016
00:53:22,000 --> 00:53:25,000
So, we are trying to do this part.

1017
00:53:25,000 --> 00:53:28,000
I cannot criticize the Ministry,

1018
00:53:28,000 --> 00:53:33,000
but I would like to see a little more proactivity from their side.

1019
00:53:35,000 --> 00:53:38,000
We have one last question.

1020
00:53:38,000 --> 00:53:42,000
We have the privilege to be listened to by Slovak speakers,

1021
00:53:43,000 --> 00:53:48,000
by the developers of the widest range of knowledge.

1022
00:53:50,000 --> 00:53:55,000
Do you have an opportunity to invite them, to help them?

1023
00:53:57,000 --> 00:54:00,000
Do you need help from the community,

1024
00:54:00,000 --> 00:54:05,000
and if so, how can you help them with all this effort?

1025
00:54:06,000 --> 00:54:11,000
If I can say something, I would like to say that in our surroundings,

1026
00:54:11,000 --> 00:54:16,000
and this is not about some private things that someone would share with us,

1027
00:54:16,000 --> 00:54:20,000
but it is about, I do not know, that we get funding from NTK,

1028
00:54:20,000 --> 00:54:23,000
that is, from our national libraries,

1029
00:54:23,000 --> 00:54:27,000
or we get it from RTV, or we get it from the judiciary, etc.

1030
00:54:27,000 --> 00:54:32,000
I think that it is very important to understand that Slovakia

1031
00:54:32,000 --> 00:54:37,000
is on the wrong path, and that it will stay on the wrong path.

1032
00:54:41,000 --> 00:54:44,000
I work for a health care provider,

1033
00:54:44,000 --> 00:54:48,000
because of my knowledge of the language and so on,

1034
00:54:48,000 --> 00:54:51,000
and they say, I was in America,

1035
00:54:51,000 --> 00:54:54,000
and a health care provider goes there for a visit,

1036
00:54:54,000 --> 00:54:58,000
and when he comes back, he has a record of what he talked to the patient.

1037
00:54:58,000 --> 00:55:01,000
I understand, but we do not have to help each other,

1038
00:55:01,000 --> 00:55:05,000
we do not have to record the conversation between the patient and the health care provider,

1039
00:55:05,000 --> 00:55:08,000
but we need to be able to come to an agreement.

1040
00:55:08,000 --> 00:55:12,000
The only way to understand a little better what needs to be done,

1041
00:55:12,000 --> 00:55:15,000
and where we need to forgive some mistakes,

1042
00:55:15,000 --> 00:55:18,000
is to do it ourselves.

1043
00:55:18,000 --> 00:55:21,000
Otherwise, it simply does not work.

1044
00:55:21,000 --> 00:55:25,000
I already told you that we have set up a thousand hours for language recognition.

1045
00:55:25,000 --> 00:55:28,000
The best models are made in 500,000 hours.

1046
00:55:28,000 --> 00:55:31,000
I already told you that.

1047
00:55:31,000 --> 00:55:34,000
Ok, I agree.

1048
00:55:37,000 --> 00:55:42,000
I will take this opportunity to tell my colleagues a little about the problems,

1049
00:55:42,000 --> 00:55:45,000
so that this conversation expands a little.

1050
00:55:48,000 --> 00:55:51,000
I would like to thank everyone for your attention,

1051
00:55:51,000 --> 00:55:56,000
both in the audience and those who are listening to us in the car.

1052
00:55:59,000 --> 00:56:04,000
It seems to me that attention is a rare goodness,

1053
00:56:04,000 --> 00:56:07,000
for which we are very grateful.

1054
00:56:09,000 --> 00:56:13,000
Thank you very much for your presence and for your many questions.

1055
00:56:22,000 --> 00:56:25,000
Thank you from my perspective, thank you to the team,

1056
00:56:25,000 --> 00:56:28,000
thank you to the supporters and thank you for being with us.

1057
00:56:28,000 --> 00:56:31,000
See you next time. Good luck!